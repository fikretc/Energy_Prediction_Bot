{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ Lukman copyright \n",
    "# MIT Licence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data frame analysis\n",
    "import pandas as pd \n",
    "\n",
    "# for mathematical operations\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# matplotlib library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For Normalizing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical test\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Split data set into training and test set\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "# SVN module\n",
    "from sklearn import svm\n",
    "\n",
    "# Kernel Functions used \n",
    "from sklearn.metrics.pairwise import rbf_kernel,laplacian_kernel\n",
    "\n",
    "# module for chi square test\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "\n",
    "# For dictionary \n",
    "from collections import defaultdict\n",
    "\n",
    "# for use of tensorflow\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "#from tensorflow.nn.rnn import *\n",
    "from tensorflow.python.ops  import *\n",
    "\n",
    "# for scaling arrays\n",
    "from sklearn.preprocessing import MaxAbsScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "# for random sampling of validation set\n",
    "import random\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "from utility import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager executinon\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# load config file\n",
    "try:\n",
    "    with open(\"../config.yml\", 'r') as ymlfile:\n",
    "            cfg = yaml.safe_load(ymlfile)\n",
    "except (IOError):\n",
    "    print('config file is required. Put config file in current directory')\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current working directory\n",
    "cwd = os.getcwd()\n",
    "# set the base directory. base directo\n",
    "BASE_DIR = os.path.join( os.path.dirname( cwd), '' )\n",
    "# cleaned data dir\n",
    "cleanedpath= BASE_DIR + cfg['cleanedconfig']['cleanedDataV1']['cleanedDir'] \n",
    "# clean data Name\n",
    "cleandataName = cfg['cleanedconfig']['cleanedDataV1']['cleanedName']\n",
    "# traina and validation data name\n",
    "trainName= cfg['cleanedconfig']['cleanedDataV1']['TensorflowDataFlowConfig']['tfcleanTrain']\n",
    "validateName  = cfg['cleanedconfig']['cleanedDataV1']['TensorflowDataFlowConfig']['tfcleanValidate']\n",
    "validation_split = cfg['cleanedconfig']['cleanedDataV1']['validationSplit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean for preprocessing datasets for tensorflow\n",
    "tfreprocess = cfg['cleanedconfig']['cleanedDataV1']['TensorflowDataFlowConfig']['tfprocess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfreprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Fikret\\\\Documents\\\\GitHub\\\\Energy_Prediction_Bot\\\\Dataset/trainData/cleaned/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cleanedDataV1.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleandataName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tftrains.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tfvals.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validateName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meter_reading', 'square_feet', 'air_temperature', 'dew_temperature', 'month', 'day', 'hour', 'primary_use_Education', 'primary_use_Entertainment/public assembly', 'primary_use_Food sales and service', 'primary_use_Healthcare', 'primary_use_Lodging/residential', 'primary_use_Manufacturing/industrial', 'primary_use_Office', 'primary_use_Other', 'primary_use_Parking', 'primary_use_Public services', 'primary_use_Religious worship', 'primary_use_Retail', 'primary_use_Technology/science', 'primary_use_Utility', 'site_id_0', 'site_id_2', 'site_id_6', 'site_id_7', 'site_id_9', 'site_id_10', 'site_id_11', 'site_id_13', 'site_id_14', 'site_id_15']\n",
      "[tf.float64, tf.int32, tf.float64, tf.float64, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32]\n",
      "==========***train option**=========================\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if tfreprocess:\n",
    "    \n",
    "    _,_,data_types,col_names= utils.tfdatabuilder(cleanedpath,cleandataName,trainName, validateName,validation_split )\n",
    "    \n",
    "else:\n",
    "    train = pd.read_csv(cleanedpath + trainName) \n",
    "    data_types,col_names = utils.tfpreprocess(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with downloaded csv to preserve data type\n",
    "Load the csv file and parse the data types of the datatset together into the\n",
    "tensorflow load pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data using the tensorflow Dataset API\n",
    "# add the data types too\n",
    "csvData = tf.data.experimental.CsvDataset(cleanedpath + trainName, data_types, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features to use \n",
    "catfeatures = cfg['cleanedconfig']['cleanedDataV1']['TensorflowDataFlowConfig']['categoricalFeatureToUse']\n",
    "catNames = cfg['cleanedconfig']['cleanedDataV1']['TensorflowDataFlowConfig']['CategoricalFeature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month_day_hour', 'primary_use', 'site_id']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meter_reading',\n",
       " 'square_feet',\n",
       " 'air_temperature',\n",
       " 'dew_temperature',\n",
       " 'month',\n",
       " 'day',\n",
       " 'hour',\n",
       " 'primary_use_Education',\n",
       " 'primary_use_Entertainment/public assembly',\n",
       " 'primary_use_Food sales and service',\n",
       " 'primary_use_Healthcare',\n",
       " 'primary_use_Lodging/residential',\n",
       " 'primary_use_Manufacturing/industrial',\n",
       " 'primary_use_Office',\n",
       " 'primary_use_Other',\n",
       " 'primary_use_Parking',\n",
       " 'primary_use_Public services',\n",
       " 'primary_use_Religious worship',\n",
       " 'primary_use_Retail',\n",
       " 'primary_use_Technology/science',\n",
       " 'primary_use_Utility',\n",
       " 'site_id_0',\n",
       " 'site_id_2',\n",
       " 'site_id_6',\n",
       " 'site_id_7',\n",
       " 'site_id_9',\n",
       " 'site_id_10',\n",
       " 'site_id_11',\n",
       " 'site_id_13',\n",
       " 'site_id_14',\n",
       " 'site_id_15']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month_day_hour', 'primary_use', 'site_id']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the data and\n",
    "# merging the columns:\n",
    "# ---- day and month as single column\n",
    "# ---- merge the site id as a single feature vector vector\n",
    "# ---- primary use as a single feature vector\n",
    "\n",
    "# take the forst 4 col names that meter reading, square feet, air temp , dew temp,\n",
    "# add it to a the new column names that would be formed after single column merging\n",
    "\n",
    "#categorical feature selection\n",
    "# here one can decide if to use site id or not . Site id is index 2 from catnames\n",
    "if catfeatures == 3:\n",
    "    col_names_= [col_names[0],col_names[1],col_names[2],col_names[3],catNames[0],catNames[1],catNames[2] ]\n",
    "else:\n",
    "    col_names_= [col_names[0],col_names[1],col_names[2],col_names[3],catNames[0],catNames[1]]\n",
    "\n",
    "def _parse_csv_row(*vals):\n",
    "    '''\n",
    "    Uses Feature columns\n",
    "    Does feature engineering\n",
    "    '''\n",
    "    \n",
    "    # month and day and single feature\n",
    "    month_day = tf.convert_to_tensor(vals[4:7])\n",
    "    # primary use as single feature\n",
    "    primary_use = tf.convert_to_tensor(vals[7:21]) # this index can be chage but ensure consistency \n",
    "    if catfeatures == 3:\n",
    "        # site id as single feature\n",
    "        site_id =  tf.convert_to_tensor(vals[21:31])\n",
    "    \n",
    "        # merge the features together, note meter reading is the fist column\n",
    "        # so it is excluded -- meaning index starts from 1\n",
    "        feature_vals = vals[1:4] + (month_day,primary_use,site_id)\n",
    "    else:\n",
    "        feature_vals = vals[1:4] + (month_day,primary_use)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # zip into a  feature sets into a single col\n",
    "    features = dict(zip(col_names_[1:],feature_vals))\n",
    "    \n",
    "    # name the targets or lables\n",
    "    targets_tensor = tf.convert_to_tensor(vals[0],name=col_names_[0]) \n",
    "    \n",
    "    \n",
    "    \n",
    "    return features, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meter_reading',\n",
       " 'square_feet',\n",
       " 'air_temperature',\n",
       " 'dew_temperature',\n",
       " 'month_day_hour',\n",
       " 'primary_use',\n",
       " 'site_id']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch the datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is needed for testing\n",
    "dataset = csvData.map(_parse_csv_row).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({square_feet: (?,), air_temperature: (?,), dew_temperature: (?,), month_day_hour: (?, 3), primary_use: (?, 14), site_id: (?, 10)}, (?,)), types: ({square_feet: tf.int32, air_temperature: tf.float64, dew_temperature: tf.float64, month_day_hour: tf.int32, primary_use: tf.int32, site_id: tf.int32}, tf.float64)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_use = tf.feature_column.numeric_column(catNames[1], shape=(14,))\n",
    "site_id = tf.feature_column.numeric_column(catNames[2], shape=(10,))\n",
    "month_day = tf.feature_column.numeric_column(catNames[0], shape=(3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumericColumn(key='primary_use', shape=(14,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n",
      "NumericColumn(key='site_id', shape=(10,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n",
      "NumericColumn(key='month_day_hour', shape=(3,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n"
     ]
    }
   ],
   "source": [
    "print(primary_use)\n",
    "print (site_id)\n",
    "print(month_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "numeric_columns = [tf.feature_column.numeric_column(feat) for feat in col_names_[1:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='square_feet', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='air_temperature', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='dew_temperature', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = numeric_columns + [month_day,primary_use,site_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_layer = tf.keras.layers.DenseFeatures(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to serve the data set \n",
    "def load_data(*filename,fn = _parse_csv_row,training=True):\n",
    "    Training = training\n",
    "    batchsize = 64\n",
    "    csvData = tf.data.experimental.CsvDataset(filename, data_types, header=True)\n",
    "    dataset= csvData.map(fn)\n",
    "    if Training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    return dataset.batch(batchsize)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def root_mean_squared_error():\n",
    "    '''\n",
    "    \n",
    "    this is the loss function specified in kaggle competition\n",
    "    '''\n",
    "        \n",
    "    return K.sqrt(K.mean (K.square( (K.log(K.abs(y_pred)+1) - K.log(y_true+1))   )   )   )\n",
    "\n",
    "\n",
    "\n",
    "def root_mean_squared_error2():\n",
    "    '''\n",
    "    this is 1.- mean squared\n",
    "    root mean squared is a value between 0 and 1 \n",
    "    to reflect accuracy use 1-rms\n",
    "    '''\n",
    "        \n",
    "    return  K.sqrt(K.mean( ( K.square ( y_pred - y_true    )   )   )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide and deep model \n",
    "# https://arxiv.org/pdf/1606.07792.pdf\n",
    "model = tf.estimator.DNNLinearCombinedRegressor(\n",
    "          # wide settings\n",
    "         linear_feature_columns=[month_day, primary_use,site_id ],\n",
    "         linear_optimizer=tf.train.FtrlOptimizer(1e-6,l2_regularization_strength=1e-8 ),\n",
    "         # deep settings\n",
    "         dnn_feature_columns=[\n",
    "           primary_use, site_id,\n",
    "           numeric_columns[0],numeric_columns[1],numeric_columns[2]],\n",
    "        dnn_hidden_units=[600, 300, 100],\n",
    "        dnn_dropout=0.3,\n",
    "        dnn_optimizer=tf.train.ProximalAdagradOptimizer(1e-6,l2_regularization_strength=1e-10)\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the metrics to use in the computation\n",
    "model = tf.contrib.estimator.add_metrics(model, root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run training\n",
    "\n",
    "for i in range(3):\n",
    "    model.train(input_fn=lambda : load_data(cleanedpath + trainName), steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "please skip testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_csv_row2(*vals):\n",
    "    '''\n",
    "    Uses Feature columns\n",
    "    Does feature engineering\n",
    "    '''\n",
    "    \n",
    "    # month and day and single feature\n",
    "    month_day = tf.convert_to_tensor(vals[4:7])\n",
    "    # primary use as single feature\n",
    "    primary_use = tf.convert_to_tensor(vals[7:21])\n",
    "    # site id as single feature\n",
    "    site_id =  tf.convert_to_tensor(vals[21:30])\n",
    "    \n",
    "    # merge the features together, note meter reading is the fist column\n",
    "    # so it is excluded -- meaning index starts from 1\n",
    "    feature_vals = vals[1:4] + (month_day,primary_use,site_id)\n",
    "    \n",
    "    \n",
    "    # zip into a  feature sets into a single col\n",
    "    features = dict(zip(col_names_[1:],feature_vals))\n",
    "    \n",
    "    # name the targets or lables\n",
    "    targets_tensor = tf.convert_to_tensor(vals[0],name=col_names_[0]) \n",
    "    \n",
    "    \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to serve the data set \n",
    "def load_data2(*filename,fn = _parse_csv_row2):\n",
    "    batchsize = 64\n",
    "    csvData = tf.data.experimental.CsvDataset(filename, data_types, header=False)\n",
    "    dataset= csvData.map(fn)\n",
    "    return dataset.batch(batchsize)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #model.train(input_fn=lambda : load_data('meter1.csv'), steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(input_fn=lambda : load_data2('meter1Test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results   = list(predictions);\n",
    "#tf.logging.info(results)\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted values from list\n",
    "pred = []\n",
    "for i in results:\n",
    "    pred.append( i['predictions'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true values\n",
    "true_values=pd.read_csv('meter1Test.csv',header=None)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred,\"-g\" ,label='Predictions')\n",
    "#plt.plot(true_values, \"-r\",label='True Values')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title('Model Prediction ')\n",
    "plt.ylabel('meter readinds')\n",
    "plt.xlabel('Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(predictions,\"-g\" ,label='Predictions')\n",
    "plt.plot(true_values, \"-r\",label='True Values',color='orange')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title('True Values ')\n",
    "plt.ylabel('meter readinds')\n",
    "plt.xlabel('Iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot per Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = pd.DataFrame(list(zip(test_building_id,test_timestamp,pred)),columns= ['building_id','timestamp', 'meter_reading' ,] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_readings = pd.DataFrame(list(zip(test_building_id,test_timestamp,list(true_values))),columns= ['building_id','timestamp', 'meter_reading' ,] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_meter(true_readings,test_predicted,n_plots=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = cfg['savedModel']['myname']['Dir3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresample = list(dataset.take(1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input function\n",
    "inut_receive_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(featuresample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export_saved_model(export_dir_base=checkpoint_path ,serving_input_receiver_fn=inut_receive_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
